{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2750aa3d-42b5-4c6e-8420-04647ffadcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7228, 4.7963, 5.2345],\n",
      "        [2.8704, 3.6979, 0.5559],\n",
      "        [1.0244, 3.8209, 1.2268]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Create a tensor with requires_grad set True\n",
    "x = torch.randn(3, 3, requires_grad=True)\n",
    "y = x**2 + 3*x + 5 \n",
    "y.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22165764-652c-4aa4-9b30-00445fe97bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x: tensor(6.)\n",
      "Gradient of w: tensor([-22.5488])\n",
      "Gradient of b: tensor(-11.2744)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Create a tensor 'x' with the value 2.0 and enable gradient tracking.\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# 2. Define a simple quadratic function: y = x^2 + 2*x + 1\n",
    "y = x**2 + 2*x + 1\n",
    "\n",
    "# 3. Compute the gradient of 'y' with respect to 'x'.\n",
    "#    Before doing this, what do you expect the gradient to be based on calculus?\n",
    "#    (The derivative of x^2 + 2x + 1 is 2x + 2. At x=2, the gradient should be 2*(2) + 2 = 6)\n",
    "\n",
    "# 4. Call the .backward() method on 'y' to compute the gradient.\n",
    "y.backward()\n",
    "\n",
    "# 5. Access the computed gradient of 'x' using the .grad attribute.\n",
    "print(\"Gradient of x:\", x.grad)\n",
    "\n",
    "# 6. Create another tensor 'w' with a random value and enable gradient tracking.\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# 7. Create a tensor 'b' with the value -1.0 and enable gradient tracking.\n",
    "b = torch.tensor(-1.0, requires_grad=True)\n",
    "\n",
    "# 8. Define a simple linear model output: prediction = w * x + b\n",
    "prediction = w * x + b\n",
    "\n",
    "# 9. Define a target value: target = 5.0\n",
    "target = torch.tensor(5.0)\n",
    "\n",
    "# 10. Calculate the mean squared error (MSE) \n",
    "loss = (prediction - target)**2  \n",
    "\n",
    "# To compute gradients for non-scalar tensors, we typically compute the gradient of a scalar output.\n",
    "# In this case, 'loss' is already a scalar.\n",
    "loss.backward()\n",
    "\n",
    "# 12. Access and print the gradients of 'w' and 'b'.\n",
    "print(\"Gradient of w:\", w.grad)\n",
    "print(\"Gradient of b:\", b.grad)\n",
    "\n",
    "# 13. Briefly explain in the comments what the .backward() method does and why it's crucial for training neural networks.\n",
    "# Your explanation here:\n",
    "# The .backward() method in PyTorch calculates the gradient of a tensor with respect to all the tensors that have requires_grad=True and were involved in the computation of that tensor.\n",
    "# It traverses the computational graph backwards from the tensor on which it's called, applying the chain rule of calculus to compute these gradients.\n",
    "# This is crucial for training neural networks because it allows us to determine how much each parameter (weight and bias) contributes to the loss.\n",
    "# These gradients are then used by optimization algorithms (like gradient descent) to update the parameters in a direction that reduces the loss, thus improving the network's performance over iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8404a1e-578d-4088-b9db-bf14ae8ea981",
   "metadata": {},
   "source": [
    "## Building Efficient Data Pipelines with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ce0cc0-004f-4c5d-bd91-9c23c2fe4dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 100\n",
      "First sample input shape: torch.Size([5])\n",
      "First sample label shape: torch.Size([])\n",
      "First sample: (tensor([-0.6893, -0.9583,  0.4137,  1.5224, -0.4382]), tensor(1.3775))\n",
      "Training dataset size: 80\n",
      "Testing dataset size: 20\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "# 1. Create sample input data (X) as a PyTorch tensor with 100 samples and 5 features (random values).\n",
    "X = torch.randn(100, 5)\n",
    "\n",
    "# 2. Create sample output labels (y) as a PyTorch tensor with 100 labels (random values).\n",
    "y = torch.randn(100)\n",
    "\n",
    "# 3. Define a custom Dataset class called 'MySampleDataset' that inherits from torch.utils.data.Dataset.\n",
    "class MySampleDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        # Initialize the dataset with features and labels.\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        assert len(self.features) == len(self.labels), \"Number of samples must match!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset.\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve the feature and label at the given index.\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return feature, label\n",
    "\n",
    "# 4. Create an instance of your 'MySampleDataset' using the sample data X and y.\n",
    "dataset = MySampleDataset(X, y)\n",
    "\n",
    "# 5. Print the size of the dataset using the __len__() method.\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "\n",
    "# 6. Access and print the first sample (input and label) from the dataset using indexing.\n",
    "first_sample_input, first_sample_label = dataset[0]\n",
    "print(\"First sample input shape:\", first_sample_input.shape)\n",
    "print(\"First sample label shape:\", first_sample_label.shape)\n",
    "print(\"First sample:\", (first_sample_input, first_sample_label))\n",
    "\n",
    "# 7. Split the dataset into training and testing sets with an 80/20 ratio.\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 8. Print the sizes of the training and testing datasets.\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Testing dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16bdb5a-b2d9-478c-a2b4-8bcaad1bc3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PyMOL2]",
   "language": "python",
   "name": "conda-env-PyMOL2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
