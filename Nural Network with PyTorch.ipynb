{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15edab3d-b994-4c1c-899c-f1530bc2fb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 100\n",
      "First sample:\n",
      "Input shape: torch.Size([5])\n",
      "Label shape: torch.Size([])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_30316\\3522980088.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_30316\\3522980088.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.y = torch.tensor(y, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with input features and labels\n",
    "        \n",
    "        Args:\n",
    "        X (torch.Tensor): Input features\n",
    "            Shape: [num_samples, num_features]\n",
    "        y (torch.Tensor): Target labels\n",
    "            Shape: [num_samples]\n",
    "        \"\"\"\n",
    "        super(CustomDataset, self).__init__()  # Changed from SimpleDataset to CustomDataset\n",
    "        # Convert inputs to tensor if they aren't already\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        # Validate that number of samples match\n",
    "        assert len(self.X) == len(self.y), \"Number of samples must match\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieve a single sample from the dataset\n",
    "        \n",
    "        Args:\n",
    "        index (int): Index of the sample to retrieve\n",
    "        \n",
    "        Returns:\n",
    "        tuple: (input_features, label)\n",
    "            input_features Shape: [num_features]\n",
    "            label Shape: [1]\n",
    "        \"\"\"\n",
    "        # Convert inputs to tensor if they aren't already\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset\n",
    "        \n",
    "        Returns:\n",
    "        int: Number of samples\n",
    "        \"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "# Example usage\n",
    "# Create sample data\n",
    "X = torch.randn(100, 5)  # 100 samples, 5 features\n",
    "y = torch.randn(100)     # 100 labels\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = CustomDataset(X, y)\n",
    "\n",
    "# Demonstrate dataset usage\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"First sample:\")\n",
    "first_input, first_label = dataset[0]\n",
    "print(\"Input shape:\", first_input.shape)\n",
    "print(\"Label shape:\", first_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09cb3d-629c-497e-96ac-1a55fff59502",
   "metadata": {},
   "source": [
    "## Building Nural Network with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "197d7a98-0497-48e9-a1e3-3eeb4df8eeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Shape of the dummy output: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. Determine if a CUDA-enabled GPU is available and set the device accordingly.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Define a simple feed-forward neural network class called 'SimpleNet' that inherits from nn.Module.\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        # Define the first fully connected (linear) layer: input_size to hidden_size.\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        # Define the activation function (e.g., nn.Tanh()).\n",
    "        self.tanh = nn.Tanh()\n",
    "        # Define the second fully connected (linear) layer: hidden_size to output_size.\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass of the network:\n",
    "        # 1. Pass the input 'x' through the first linear layer.\n",
    "        out = self.linear1(x)\n",
    "        # 2. Apply the activation function.\n",
    "        out = self.tanh(out)\n",
    "        # 3. Pass the output through the second linear layer.\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# 3. Instantiate the 'SimpleNet' model with an input size of 1, a hidden size of 10, and an output size of 1.\n",
    "input_size = 1\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "model = SimpleNet(input_size, hidden_size, output_size)\n",
    "\n",
    "# 4. Move the model to the defined device (CPU or GPU).\n",
    "model.to(device)\n",
    "\n",
    "# 5. Generate a dummy input tensor of shape (1, 1) and move it to the same device as the model.\n",
    "dummy_input = torch.randn(1, 1).to(device)\n",
    "\n",
    "# 6. Perform a forward pass with the dummy input and print the output shape.\n",
    "dummy_output = model(dummy_input)\n",
    "print(\"Shape of the dummy output:\", dummy_output.shape)\n",
    "\n",
    "# 7. Briefly explain in the comments the role of the activation function (self.tanh) in this network.\n",
    "# Your explanation here:\n",
    "# The activation function (tanh in this case) introduces non-linearity into the neural network.\n",
    "# Without non-linear activation functions, a neural network with multiple linear layers would be mathematically equivalent to a single linear layer, limiting its ability to learn complex, non-linear relationships in the data.\n",
    "# The tanh function allows the network to model more intricate patterns by transforming the linear outputs of the first layer before they are passed to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd25b9-54cc-4c60-894c-2b72bd84ee70",
   "metadata": {},
   "source": [
    "## Choosing and Using Loss functions in PyTrorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7db4f1-1392-431d-aa49-490e2af8c9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Loss: tensor(0.6000)\n",
      "MSE Loss: tensor(0.4000)\n",
      "L1 Loss with outlier: tensor(4.)\n",
      "MSE Loss with outlier: tensor(61.6000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Define a set of example model predictions (y_pred) and true values (y_true) as PyTorch tensors.\n",
    "y_pred = torch.tensor([2.0, 4.0, 6.0, 8.0, 10.0])\n",
    "y_true = torch.tensor([2.5, 3.5, 6.5, 7.5, 9.0])\n",
    "\n",
    "# 2. Instantiate the L1 Loss function from PyTorch.\n",
    "l1_loss_function = nn.L1Loss()\n",
    "\n",
    "# 3. Calculate the L1 Loss between y_pred and y_true.\n",
    "l1_loss = l1_loss_function(y_pred, y_true)\n",
    "print(\"L1 Loss:\", l1_loss)\n",
    "\n",
    "# 4. Instantiate the MSE Loss function from PyTorch.\n",
    "mse_loss_function = nn.MSELoss()\n",
    "\n",
    "# 5. Calculate the MSE Loss between y_pred and y_true.\n",
    "mse_loss = mse_loss_function(y_pred, y_true)\n",
    "print(\"MSE Loss:\", mse_loss)\n",
    "\n",
    "# 6. Now, let's introduce an outlier in our predictions. Modify the first element of y_pred to 20.0.\n",
    "y_pred_with_outlier = torch.tensor([20.0, 4.0, 6.0, 8.0, 10.0])\n",
    "\n",
    "# 7. Calculate the L1 Loss with the outlier.\n",
    "l1_loss_with_outlier = l1_loss_function(y_pred_with_outlier, y_true)\n",
    "print(\"L1 Loss with outlier:\", l1_loss_with_outlier)\n",
    "\n",
    "# 8. Calculate the MSE Loss with the outlier.\n",
    "mse_loss_with_outlier = mse_loss_function(y_pred_with_outlier, y_true)\n",
    "print(\"MSE Loss with outlier:\", mse_loss_with_outlier)\n",
    "\n",
    "# 9. Based on the results, briefly explain in the comments the difference in how L1 Loss and MSE Loss are affected by the outlier.\n",
    "# Your explanation here:\n",
    "# L1 Loss calculates the absolute difference, so the impact of the outlier is linear. The increase in L1 Loss due to the outlier is directly proportional to the size of the outlier.\n",
    "# MSE Loss calculates the squared difference, so the impact of the outlier is quadratic. The increase in MSE Loss due to the outlier is much larger compared to L1 Loss because the error is squared, heavily penalizing the large error introduced by the outlier.\n",
    "# This demonstrates that MSE Loss is more sensitive to outliers than L1 Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8179a3c-487c-4916-8e46-aa7d61f8e512",
   "metadata": {},
   "source": [
    "## Implementind a simple Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5171517-d7ac-493b-b020-d84a35bce35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 23.6540\n",
      "Epoch [2/10], Loss: 16.4611\n",
      "Epoch [3/10], Loss: 11.4699\n",
      "Epoch [4/10], Loss: 8.0062\n",
      "Epoch [5/10], Loss: 5.6026\n",
      "Epoch [6/10], Loss: 3.9345\n",
      "Epoch [7/10], Loss: 2.7768\n",
      "Epoch [8/10], Loss: 1.9731\n",
      "Epoch [9/10], Loss: 1.4153\n",
      "Epoch [10/10], Loss: 1.0279\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight: tensor([[1.4212]])\n",
      "linear.bias: tensor([0.8638])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Define a simple linear regression model.\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 2. Create some dummy training data (features X and labels y).\n",
    "X_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]], dtype=torch.float32)\n",
    "y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0]], dtype=torch.float32)\n",
    "\n",
    "# 3. Instantiate the LinearRegression model with input and output dimensions of 1.\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "model = LinearRegression(input_dim, output_dim)\n",
    "\n",
    "# 4. Define the loss function (Mean Squared Error Loss).\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 5. Define the optimizer (Stochastic Gradient Descent) with a learning rate of 0.01.\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 6. Implement the training loop. Run for a few epochs (e.g., 10).\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass: compute predictions on the training data.\n",
    "    outputs = model(X_train)\n",
    "\n",
    "    # Calculate the loss between the predictions and the true labels.\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward pass: compute the gradients of the loss with respect to the model parameters.\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()       # Compute gradients\n",
    "    optimizer.step()      # Update model parameters\n",
    "\n",
    "    # Print the loss for each epoch to observe the training progress.\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 7. After training, print the learned parameters (weights and bias) of the linear layer.\n",
    "print(\"\\nLearned parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007db77-86fa-45de-8748-4353fc6b1cb1",
   "metadata": {},
   "source": [
    "## Regression Metrics Scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67adbbb3-9c40-4598-92c0-1d6521cfd2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_curve, auc, precision_recall_curve, confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea6b7fc4-d339-48c7-8564-dee1fc2eaea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Regression Metrics:\n",
      "MAE: 0.614\n",
      "R2: 0.936\n",
      "\n",
      "Selected Classification Metrics:\n",
      "Precision: 0.564\n",
      "Recall: 0.509\n",
      "\n",
      "Evaluation within torch.no_grad():\n",
      "MAE (eval): 0.614\n",
      "Accuracy(eval): 0.505\n"
     ]
    }
   ],
   "source": [
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Regression data\n",
    "y_true_reg = torch.linspace(0, 10, steps=100) + torch.randn(100) * 0.5\n",
    "y_pred_reg = y_true_reg + torch.randn(100) * 0.8\n",
    "\n",
    "# Classification data\n",
    "y_true_cls = torch.randint(0, 2, (200, ))\n",
    "y_scores = torch.sigmoid(torch.randn(200))\n",
    "y_pred_cls = (y_scores > 0.5).long()\n",
    "\n",
    "# Convert to NumPy for sKlearn Metrics\n",
    "y_true_reg_np = y_true_reg.numpy()  # Fixed: lowercase y instead of uppercase Y\n",
    "y_pred_reg_np = y_pred_reg.numpy()  # Fixed: lowercase y instead of uppercase Y\n",
    "y_true_cls_np = y_true_cls.numpy()  # Fixed: lowercase y instead of uppercase Y\n",
    "y_scores_np = y_scores.numpy()      # Fixed: lowercase y instead of uppercase Y\n",
    "y_pred_cls_np = y_pred_cls.numpy()  # Added this line to convert y_pred_cls to NumPy array\n",
    "\n",
    "# 1. Calculate and explain specific regression metrics\n",
    "mae = mean_absolute_error(y_true_reg_np, y_pred_reg_np)\n",
    "r2 = r2_score(y_true_reg_np, y_pred_reg_np)\n",
    "print(\"Selected Regression Metrics:\")\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "print(f\"R2: {r2:.3f}\\n\")\n",
    "\n",
    "# Explanation\n",
    "# 2 Calculate and explain specific classification metrics\n",
    "precision = precision_score(y_true_cls_np, y_pred_cls_np, zero_division=0)\n",
    "recall = recall_score(y_true_cls_np, y_pred_cls_np, zero_division=0)\n",
    "print(\"Selected Classification Metrics:\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\\n\")\n",
    "\n",
    "# Explanation:\n",
    "# 3. Demonstrate evaluation using torch.no_read()\n",
    "#Simulate a pyTorch model's output(already done with y_pred tensors)\n",
    "with torch.no_grad():\n",
    "    # In a real scenario, this would be:\n",
    "    # model.eval()\n",
    "    # y_pred_reg_tensor = model(X_test_reg)\n",
    "    # y_pred_cls_tensor = (torch.sigmoid(model_cls(X_test_cls)) > 0.5).long()\n",
    "\n",
    "    # For this exercise, we'll use the existing tensors\n",
    "    mae_eval = mean_absolute_error(y_true_reg.numpy(), y_pred_reg.numpy())\n",
    "    accuracy_eval = accuracy_score(y_true_cls.numpy(), y_pred_cls.numpy())\n",
    "\n",
    "print(\"Evaluation within torch.no_grad():\")\n",
    "print(f\"MAE (eval): {mae_eval:.3f}\")\n",
    "print(f\"Accuracy(eval): {accuracy_eval:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d54175-07bb-44e1-9733-b8fc19bd5b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PyMOL2]",
   "language": "python",
   "name": "conda-env-PyMOL2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
